{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b56810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c267c46fbaf4458da80a2eb73311d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd39b5a04dfe4461a903907b6bcf3821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86037dbbdf84faa94f93d3e8fd952f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9151027d575496eb54083fc63e96c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST\\raw\n",
      "\n",
      "Dataloaders created\n",
      "Model created\n",
      "Train loop variables created\n",
      "Epoch 0\n",
      "\n",
      "starting epoch\n",
      "Epoch [0/3], Step [0/3750],\n",
      "train loss: 2.2940, train acc: 12.5000                             \n",
      "validation loss: 2.2985, validation acc: 22.0400\n",
      "\n",
      "Epoch [0/3], Step [10/3750],\n",
      "train loss: 1.8324, train acc: 56.2500                             \n",
      "validation loss: 2.1076, validation acc: 47.9900\n",
      "\n",
      "Epoch [0/3], Step [20/3750],\n",
      "train loss: 1.6862, train acc: 25.0000                             \n",
      "validation loss: 1.8419, validation acc: 44.6200\n",
      "\n",
      "Epoch [0/3], Step [30/3750],\n",
      "train loss: 0.8428, train acc: 87.5000                             \n",
      "validation loss: 1.6386, validation acc: 59.2400\n",
      "\n",
      "Epoch [0/3], Step [40/3750],\n",
      "train loss: 0.7910, train acc: 62.5000                             \n",
      "validation loss: 1.5039, validation acc: 64.3000\n",
      "\n",
      "Epoch [0/3], Step [50/3750],\n",
      "train loss: 0.7697, train acc: 75.0000                             \n",
      "validation loss: 1.4141, validation acc: 61.8400\n",
      "\n",
      "Epoch [0/3], Step [60/3750],\n",
      "train loss: 0.8501, train acc: 75.0000                             \n",
      "validation loss: 1.3290, validation acc: 75.0100\n",
      "\n",
      "Epoch [0/3], Step [70/3750],\n",
      "train loss: 0.8009, train acc: 62.5000                             \n",
      "validation loss: 1.2415, validation acc: 78.3400\n",
      "\n",
      "Epoch [0/3], Step [80/3750],\n",
      "train loss: 0.2852, train acc: 93.7500                             \n",
      "validation loss: 1.1724, validation acc: 76.8800\n",
      "\n",
      "Epoch [0/3], Step [90/3750],\n",
      "train loss: 0.5153, train acc: 75.0000                             \n",
      "validation loss: 1.1200, validation acc: 82.5400\n",
      "\n",
      "Epoch [0/3], Step [100/3750],\n",
      "train loss: 0.6497, train acc: 75.0000                             \n",
      "validation loss: 1.0759, validation acc: 81.3800\n",
      "\n",
      "Epoch [0/3], Step [110/3750],\n",
      "train loss: 0.3145, train acc: 93.7500                             \n",
      "validation loss: 1.0316, validation acc: 83.2100\n",
      "\n",
      "Epoch [0/3], Step [120/3750],\n",
      "train loss: 0.4580, train acc: 81.2500                             \n",
      "validation loss: 0.9909, validation acc: 83.6100\n",
      "\n",
      "Epoch [0/3], Step [130/3750],\n",
      "train loss: 0.2616, train acc: 93.7500                             \n",
      "validation loss: 0.9556, validation acc: 81.7400\n",
      "\n",
      "Epoch [0/3], Step [140/3750],\n",
      "train loss: 0.3601, train acc: 87.5000                             \n",
      "validation loss: 0.9263, validation acc: 84.3400\n",
      "\n",
      "Epoch [0/3], Step [150/3750],\n",
      "train loss: 0.2591, train acc: 87.5000                             \n",
      "validation loss: 0.9005, validation acc: 84.2000\n",
      "\n",
      "Epoch [0/3], Step [160/3750],\n",
      "train loss: 0.3035, train acc: 87.5000                             \n",
      "validation loss: 0.8802, validation acc: 82.7900\n",
      "\n",
      "Epoch [0/3], Step [170/3750],\n",
      "train loss: 0.9547, train acc: 62.5000                             \n",
      "validation loss: 0.8577, validation acc: 86.9300\n",
      "\n",
      "Epoch [0/3], Step [180/3750],\n",
      "train loss: 0.4736, train acc: 81.2500                             \n",
      "validation loss: 0.8369, validation acc: 83.5600\n",
      "\n",
      "Epoch [0/3], Step [190/3750],\n",
      "train loss: 0.6723, train acc: 87.5000                             \n",
      "validation loss: 0.8214, validation acc: 84.8500\n",
      "\n",
      "Epoch [0/3], Step [200/3750],\n",
      "train loss: 0.8167, train acc: 75.0000                             \n",
      "validation loss: 0.8045, validation acc: 86.8000\n",
      "\n",
      "Epoch [0/3], Step [210/3750],\n",
      "train loss: 0.8792, train acc: 75.0000                             \n",
      "validation loss: 0.7869, validation acc: 87.3600\n",
      "\n",
      "Epoch [0/3], Step [220/3750],\n",
      "train loss: 0.8744, train acc: 81.2500                             \n",
      "validation loss: 0.7730, validation acc: 87.6400\n",
      "\n",
      "Epoch [0/3], Step [230/3750],\n",
      "train loss: 0.8157, train acc: 81.2500                             \n",
      "validation loss: 0.7613, validation acc: 84.4500\n",
      "\n",
      "Epoch [0/3], Step [240/3750],\n",
      "train loss: 0.1131, train acc: 100.0000                             \n",
      "validation loss: 0.7523, validation acc: 85.3400\n",
      "\n",
      "Epoch [0/3], Step [250/3750],\n",
      "train loss: 0.2099, train acc: 100.0000                             \n",
      "validation loss: 0.7426, validation acc: 81.8100\n",
      "\n",
      "Epoch [0/3], Step [260/3750],\n",
      "train loss: 0.4498, train acc: 87.5000                             \n",
      "validation loss: 0.7355, validation acc: 85.7100\n",
      "\n",
      "Epoch [0/3], Step [270/3750],\n",
      "train loss: 0.6041, train acc: 81.2500                             \n",
      "validation loss: 0.7273, validation acc: 78.9500\n",
      "\n",
      "Epoch [0/3], Step [280/3750],\n",
      "train loss: 0.2562, train acc: 93.7500                             \n",
      "validation loss: 0.7210, validation acc: 86.1700\n",
      "\n",
      "Epoch [0/3], Step [290/3750],\n",
      "train loss: 0.6328, train acc: 81.2500                             \n",
      "validation loss: 0.7125, validation acc: 86.7100\n",
      "\n",
      "Epoch [0/3], Step [300/3750],\n",
      "train loss: 0.2916, train acc: 87.5000                             \n",
      "validation loss: 0.7053, validation acc: 88.2800\n",
      "\n",
      "Epoch [0/3], Step [310/3750],\n",
      "train loss: 0.5374, train acc: 81.2500                             \n",
      "validation loss: 0.6965, validation acc: 89.6700\n",
      "\n",
      "Epoch [0/3], Step [320/3750],\n",
      "train loss: 0.4763, train acc: 87.5000                             \n",
      "validation loss: 0.6878, validation acc: 86.7300\n",
      "\n",
      "Epoch [0/3], Step [330/3750],\n",
      "train loss: 0.3180, train acc: 81.2500                             \n",
      "validation loss: 0.6796, validation acc: 87.2800\n",
      "\n",
      "Epoch [0/3], Step [340/3750],\n",
      "train loss: 0.5817, train acc: 93.7500                             \n",
      "validation loss: 0.6722, validation acc: 87.7700\n",
      "\n",
      "Epoch [0/3], Step [350/3750],\n",
      "train loss: 0.4338, train acc: 81.2500                             \n",
      "validation loss: 0.6638, validation acc: 87.5300\n",
      "\n",
      "Epoch [0/3], Step [360/3750],\n",
      "train loss: 0.1063, train acc: 100.0000                             \n",
      "validation loss: 0.6561, validation acc: 88.6500\n",
      "\n",
      "Epoch [0/3], Step [370/3750],\n",
      "train loss: 0.3061, train acc: 81.2500                             \n",
      "validation loss: 0.6493, validation acc: 88.0700\n",
      "\n",
      "Epoch [0/3], Step [380/3750],\n",
      "train loss: 0.1756, train acc: 93.7500                             \n",
      "validation loss: 0.6427, validation acc: 89.8600\n",
      "\n",
      "Epoch [0/3], Step [390/3750],\n",
      "train loss: 0.7778, train acc: 87.5000                             \n",
      "validation loss: 0.6356, validation acc: 89.6000\n",
      "\n",
      "Epoch [0/3], Step [400/3750],\n",
      "train loss: 0.1720, train acc: 93.7500                             \n",
      "validation loss: 0.6287, validation acc: 89.3800\n",
      "\n",
      "Epoch [0/3], Step [410/3750],\n",
      "train loss: 0.5923, train acc: 81.2500                             \n",
      "validation loss: 0.6241, validation acc: 89.3100\n",
      "\n",
      "Epoch [0/3], Step [420/3750],\n",
      "train loss: 0.5026, train acc: 87.5000                             \n",
      "validation loss: 0.6184, validation acc: 86.8800\n",
      "\n",
      "Epoch [0/3], Step [430/3750],\n",
      "train loss: 0.0873, train acc: 100.0000                             \n",
      "validation loss: 0.6137, validation acc: 86.8300\n",
      "\n",
      "Epoch [0/3], Step [440/3750],\n",
      "train loss: 1.0735, train acc: 81.2500                             \n",
      "validation loss: 0.6084, validation acc: 89.7700\n",
      "\n",
      "Epoch [0/3], Step [450/3750],\n",
      "train loss: 0.3106, train acc: 87.5000                             \n",
      "validation loss: 0.6027, validation acc: 90.2600\n",
      "\n",
      "Epoch [0/3], Step [460/3750],\n",
      "train loss: 0.8960, train acc: 75.0000                             \n",
      "validation loss: 0.5979, validation acc: 89.7800\n",
      "\n",
      "Epoch [0/3], Step [470/3750],\n",
      "train loss: 0.0917, train acc: 100.0000                             \n",
      "validation loss: 0.5921, validation acc: 89.2600\n",
      "\n",
      "Epoch [0/3], Step [480/3750],\n",
      "train loss: 1.1822, train acc: 75.0000                             \n",
      "validation loss: 0.5871, validation acc: 90.2500\n",
      "\n",
      "Epoch [0/3], Step [490/3750],\n",
      "train loss: 0.2005, train acc: 93.7500                             \n",
      "validation loss: 0.5821, validation acc: 89.7000\n",
      "\n",
      "Epoch [0/3], Step [500/3750],\n",
      "train loss: 0.2346, train acc: 93.7500                             \n",
      "validation loss: 0.5783, validation acc: 88.0600\n",
      "\n",
      "Epoch [0/3], Step [510/3750],\n",
      "train loss: 0.0443, train acc: 100.0000                             \n",
      "validation loss: 0.5752, validation acc: 89.0300\n",
      "\n",
      "Epoch [0/3], Step [520/3750],\n",
      "train loss: 0.7552, train acc: 75.0000                             \n",
      "validation loss: 0.5718, validation acc: 90.0100\n",
      "\n",
      "Epoch [0/3], Step [530/3750],\n",
      "train loss: 0.2796, train acc: 87.5000                             \n",
      "validation loss: 0.5680, validation acc: 87.6000\n",
      "\n",
      "Epoch [0/3], Step [540/3750],\n",
      "train loss: 0.3911, train acc: 81.2500                             \n",
      "validation loss: 0.5647, validation acc: 89.7000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/3], Step [550/3750],\n",
      "train loss: 0.1943, train acc: 93.7500                             \n",
      "validation loss: 0.5615, validation acc: 88.7500\n",
      "\n",
      "Epoch [0/3], Step [560/3750],\n",
      "train loss: 0.5980, train acc: 93.7500                             \n",
      "validation loss: 0.5581, validation acc: 88.8600\n",
      "\n",
      "Epoch [0/3], Step [570/3750],\n",
      "train loss: 0.3922, train acc: 87.5000                             \n",
      "validation loss: 0.5557, validation acc: 89.4400\n",
      "\n",
      "Epoch [0/3], Step [580/3750],\n",
      "train loss: 0.2766, train acc: 93.7500                             \n",
      "validation loss: 0.5526, validation acc: 88.6400\n",
      "\n",
      "Epoch [0/3], Step [590/3750],\n",
      "train loss: 0.0326, train acc: 100.0000                             \n",
      "validation loss: 0.5513, validation acc: 87.5300\n",
      "\n",
      "Epoch [0/3], Step [600/3750],\n",
      "train loss: 0.3022, train acc: 87.5000                             \n",
      "validation loss: 0.5487, validation acc: 90.3200\n",
      "\n",
      "Epoch [0/3], Step [610/3750],\n",
      "train loss: 0.2501, train acc: 93.7500                             \n",
      "validation loss: 0.5452, validation acc: 89.9900\n",
      "\n",
      "Epoch [0/3], Step [620/3750],\n",
      "train loss: 0.2908, train acc: 93.7500                             \n",
      "validation loss: 0.5417, validation acc: 91.3200\n",
      "\n",
      "Epoch [0/3], Step [630/3750],\n",
      "train loss: 0.1955, train acc: 93.7500                             \n",
      "validation loss: 0.5376, validation acc: 92.0700\n",
      "\n",
      "Epoch [0/3], Step [640/3750],\n",
      "train loss: 0.1765, train acc: 100.0000                             \n",
      "validation loss: 0.5336, validation acc: 91.9600\n",
      "\n",
      "Epoch [0/3], Step [650/3750],\n",
      "train loss: 0.1916, train acc: 93.7500                             \n",
      "validation loss: 0.5299, validation acc: 91.2200\n",
      "\n",
      "Epoch [0/3], Step [660/3750],\n",
      "train loss: 0.7242, train acc: 81.2500                             \n",
      "validation loss: 0.5263, validation acc: 91.4700\n",
      "\n",
      "Epoch [0/3], Step [670/3750],\n",
      "train loss: 0.1687, train acc: 93.7500                             \n",
      "validation loss: 0.5236, validation acc: 88.8600\n",
      "\n",
      "Epoch [0/3], Step [680/3750],\n",
      "train loss: 0.1225, train acc: 100.0000                             \n",
      "validation loss: 0.5219, validation acc: 87.5100\n",
      "\n",
      "Epoch [0/3], Step [690/3750],\n",
      "train loss: 0.3160, train acc: 87.5000                             \n",
      "validation loss: 0.5206, validation acc: 87.2200\n",
      "\n",
      "Epoch [0/3], Step [700/3750],\n",
      "train loss: 0.3142, train acc: 87.5000                             \n",
      "validation loss: 0.5191, validation acc: 89.3500\n",
      "\n",
      "Epoch [0/3], Step [710/3750],\n",
      "train loss: 0.1384, train acc: 93.7500                             \n",
      "validation loss: 0.5173, validation acc: 88.8700\n",
      "\n",
      "Epoch [0/3], Step [720/3750],\n",
      "train loss: 0.4602, train acc: 93.7500                             \n",
      "validation loss: 0.5153, validation acc: 90.1400\n",
      "\n",
      "Epoch [0/3], Step [730/3750],\n",
      "train loss: 1.3092, train acc: 93.7500                             \n",
      "validation loss: 0.5136, validation acc: 90.3600\n",
      "\n",
      "Epoch [0/3], Step [740/3750],\n",
      "train loss: 0.8559, train acc: 81.2500                             \n",
      "validation loss: 0.5119, validation acc: 88.0100\n",
      "\n",
      "Epoch [0/3], Step [750/3750],\n",
      "train loss: 0.6121, train acc: 87.5000                             \n",
      "validation loss: 0.5106, validation acc: 89.3400\n",
      "\n",
      "Epoch [0/3], Step [760/3750],\n",
      "train loss: 0.2040, train acc: 93.7500                             \n",
      "validation loss: 0.5083, validation acc: 91.2000\n",
      "\n",
      "Epoch [0/3], Step [770/3750],\n",
      "train loss: 0.4176, train acc: 87.5000                             \n",
      "validation loss: 0.5063, validation acc: 90.4300\n",
      "\n",
      "Epoch [0/3], Step [780/3750],\n",
      "train loss: 0.3895, train acc: 93.7500                             \n",
      "validation loss: 0.5047, validation acc: 89.1300\n",
      "\n",
      "Epoch [0/3], Step [790/3750],\n",
      "train loss: 0.1541, train acc: 100.0000                             \n",
      "validation loss: 0.5025, validation acc: 91.9300\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "starting epoch\n",
      "Epoch [1/3], Step [0/3750],\n",
      "train loss: 0.0308, train acc: 100.0000                             \n",
      "validation loss: 0.4999, validation acc: 91.8500\n",
      "\n",
      "Epoch [1/3], Step [10/3750],\n",
      "train loss: 0.1604, train acc: 87.5000                             \n",
      "validation loss: 0.4979, validation acc: 91.0400\n",
      "\n",
      "Epoch [1/3], Step [20/3750],\n",
      "train loss: 0.1127, train acc: 93.7500                             \n",
      "validation loss: 0.4967, validation acc: 89.3000\n",
      "\n",
      "Epoch [1/3], Step [30/3750],\n",
      "train loss: 0.8678, train acc: 87.5000                             \n",
      "validation loss: 0.4967, validation acc: 89.9300\n",
      "\n",
      "Epoch [1/3], Step [40/3750],\n",
      "train loss: 0.2033, train acc: 87.5000                             \n",
      "validation loss: 0.4957, validation acc: 87.7200\n",
      "\n",
      "Epoch [1/3], Step [50/3750],\n",
      "train loss: 0.4513, train acc: 87.5000                             \n",
      "validation loss: 0.4948, validation acc: 88.5800\n",
      "\n",
      "Epoch [1/3], Step [60/3750],\n",
      "train loss: 0.5101, train acc: 93.7500                             \n",
      "validation loss: 0.4936, validation acc: 88.4800\n",
      "\n",
      "Epoch [1/3], Step [70/3750],\n",
      "train loss: 0.1645, train acc: 93.7500                             \n",
      "validation loss: 0.4925, validation acc: 89.9500\n",
      "\n",
      "Epoch [1/3], Step [80/3750],\n",
      "train loss: 0.2573, train acc: 87.5000                             \n",
      "validation loss: 0.4910, validation acc: 90.5300\n",
      "\n",
      "Epoch [1/3], Step [90/3750],\n",
      "train loss: 0.7971, train acc: 75.0000                             \n",
      "validation loss: 0.4894, validation acc: 89.3200\n",
      "\n",
      "Epoch [1/3], Step [100/3750],\n",
      "train loss: 0.1018, train acc: 100.0000                             \n",
      "validation loss: 0.4875, validation acc: 90.9000\n",
      "\n",
      "Epoch [1/3], Step [110/3750],\n",
      "train loss: 0.1021, train acc: 100.0000                             \n",
      "validation loss: 0.4858, validation acc: 91.7000\n",
      "\n",
      "Epoch [1/3], Step [120/3750],\n",
      "train loss: 0.2389, train acc: 87.5000                             \n",
      "validation loss: 0.4837, validation acc: 91.1500\n",
      "\n",
      "Epoch [1/3], Step [130/3750],\n",
      "train loss: 0.1060, train acc: 93.7500                             \n",
      "validation loss: 0.4817, validation acc: 91.2500\n",
      "\n",
      "Epoch [1/3], Step [140/3750],\n",
      "train loss: 0.0349, train acc: 100.0000                             \n",
      "validation loss: 0.4802, validation acc: 90.9400\n",
      "\n",
      "Epoch [1/3], Step [150/3750],\n",
      "train loss: 0.1378, train acc: 93.7500                             \n",
      "validation loss: 0.4785, validation acc: 90.1400\n",
      "\n",
      "Epoch [1/3], Step [160/3750],\n",
      "train loss: 0.2132, train acc: 93.7500                             \n",
      "validation loss: 0.4769, validation acc: 90.1200\n",
      "\n",
      "Epoch [1/3], Step [170/3750],\n",
      "train loss: 0.3721, train acc: 87.5000                             \n",
      "validation loss: 0.4762, validation acc: 90.2900\n",
      "\n",
      "Epoch [1/3], Step [180/3750],\n",
      "train loss: 0.0508, train acc: 100.0000                             \n",
      "validation loss: 0.4751, validation acc: 90.8200\n",
      "\n",
      "Epoch [1/3], Step [190/3750],\n",
      "train loss: 0.3168, train acc: 87.5000                             \n",
      "validation loss: 0.4742, validation acc: 87.9800\n",
      "\n",
      "Epoch [1/3], Step [200/3750],\n",
      "train loss: 0.2652, train acc: 93.7500                             \n",
      "validation loss: 0.4733, validation acc: 89.6200\n",
      "\n",
      "Epoch [1/3], Step [210/3750],\n",
      "train loss: 0.8094, train acc: 87.5000                             \n",
      "validation loss: 0.4725, validation acc: 90.3100\n",
      "\n",
      "Epoch [1/3], Step [220/3750],\n",
      "train loss: 0.3699, train acc: 87.5000                             \n",
      "validation loss: 0.4712, validation acc: 91.3200\n",
      "\n",
      "Epoch [1/3], Step [230/3750],\n",
      "train loss: 0.4979, train acc: 93.7500                             \n",
      "validation loss: 0.4702, validation acc: 88.5200\n",
      "\n",
      "Epoch [1/3], Step [240/3750],\n",
      "train loss: 0.2284, train acc: 87.5000                             \n",
      "validation loss: 0.4692, validation acc: 89.8300\n",
      "\n",
      "Epoch [1/3], Step [250/3750],\n",
      "train loss: 0.0342, train acc: 100.0000                             \n",
      "validation loss: 0.4679, validation acc: 91.3200\n",
      "\n",
      "Epoch [1/3], Step [260/3750],\n",
      "train loss: 0.8627, train acc: 87.5000                             \n",
      "validation loss: 0.4668, validation acc: 91.3600\n",
      "\n",
      "Epoch [1/3], Step [270/3750],\n",
      "train loss: 0.2512, train acc: 93.7500                             \n",
      "validation loss: 0.4656, validation acc: 91.8200\n",
      "\n",
      "Epoch [1/3], Step [280/3750],\n",
      "train loss: 0.2087, train acc: 93.7500                             \n",
      "validation loss: 0.4642, validation acc: 91.8900\n",
      "\n",
      "Epoch [1/3], Step [290/3750],\n",
      "train loss: 0.4129, train acc: 87.5000                             \n",
      "validation loss: 0.4629, validation acc: 91.8200\n",
      "\n",
      "Epoch [1/3], Step [300/3750],\n",
      "train loss: 0.0102, train acc: 100.0000                             \n",
      "validation loss: 0.4614, validation acc: 91.5700\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [310/3750],\n",
      "train loss: 0.2784, train acc: 87.5000                             \n",
      "validation loss: 0.4602, validation acc: 90.0700\n",
      "\n",
      "Epoch [1/3], Step [320/3750],\n",
      "train loss: 0.9943, train acc: 68.7500                             \n",
      "validation loss: 0.4599, validation acc: 90.5100\n",
      "\n",
      "Epoch [1/3], Step [330/3750],\n",
      "train loss: 0.0863, train acc: 100.0000                             \n",
      "validation loss: 0.4585, validation acc: 91.6700\n",
      "\n",
      "Epoch [1/3], Step [340/3750],\n",
      "train loss: 0.8788, train acc: 87.5000                             \n",
      "validation loss: 0.4579, validation acc: 90.7500\n",
      "\n",
      "Epoch [1/3], Step [350/3750],\n",
      "train loss: 0.3220, train acc: 87.5000                             \n",
      "validation loss: 0.4566, validation acc: 91.7500\n",
      "\n",
      "Epoch [1/3], Step [360/3750],\n",
      "train loss: 0.1135, train acc: 100.0000                             \n",
      "validation loss: 0.4556, validation acc: 91.0500\n",
      "\n",
      "Epoch [1/3], Step [370/3750],\n",
      "train loss: 0.2450, train acc: 87.5000                             \n",
      "validation loss: 0.4543, validation acc: 91.6000\n",
      "\n",
      "Epoch [1/3], Step [380/3750],\n",
      "train loss: 0.4896, train acc: 87.5000                             \n",
      "validation loss: 0.4532, validation acc: 90.1500\n",
      "\n",
      "Epoch [1/3], Step [390/3750],\n",
      "train loss: 0.2454, train acc: 93.7500                             \n",
      "validation loss: 0.4519, validation acc: 92.2700\n",
      "\n",
      "Epoch [1/3], Step [400/3750],\n",
      "train loss: 0.1617, train acc: 93.7500                             \n",
      "validation loss: 0.4507, validation acc: 91.7900\n",
      "\n",
      "Epoch [1/3], Step [410/3750],\n",
      "train loss: 0.2246, train acc: 93.7500                             \n",
      "validation loss: 0.4495, validation acc: 92.0400\n",
      "\n",
      "Epoch [1/3], Step [420/3750],\n",
      "train loss: 0.2303, train acc: 93.7500                             \n",
      "validation loss: 0.4481, validation acc: 92.4700\n",
      "\n",
      "Epoch [1/3], Step [430/3750],\n",
      "train loss: 0.1361, train acc: 100.0000                             \n",
      "validation loss: 0.4469, validation acc: 91.7000\n",
      "\n",
      "Epoch [1/3], Step [440/3750],\n",
      "train loss: 0.1833, train acc: 93.7500                             \n",
      "validation loss: 0.4455, validation acc: 92.6700\n",
      "\n",
      "Epoch [1/3], Step [450/3750],\n",
      "train loss: 0.0336, train acc: 100.0000                             \n",
      "validation loss: 0.4442, validation acc: 92.5500\n",
      "\n",
      "Epoch [1/3], Step [460/3750],\n",
      "train loss: 0.1257, train acc: 93.7500                             \n",
      "validation loss: 0.4428, validation acc: 92.8900\n",
      "\n",
      "Epoch [1/3], Step [470/3750],\n",
      "train loss: 0.8789, train acc: 75.0000                             \n",
      "validation loss: 0.4416, validation acc: 93.0200\n",
      "\n",
      "Epoch [1/3], Step [480/3750],\n",
      "train loss: 1.7417, train acc: 87.5000                             \n",
      "validation loss: 0.4404, validation acc: 92.1500\n",
      "\n",
      "Epoch [1/3], Step [490/3750],\n",
      "train loss: 0.0191, train acc: 100.0000                             \n",
      "validation loss: 0.4392, validation acc: 92.6900\n",
      "\n",
      "Epoch [1/3], Step [500/3750],\n",
      "train loss: 0.2280, train acc: 93.7500                             \n",
      "validation loss: 0.4380, validation acc: 92.9900\n",
      "\n",
      "Epoch [1/3], Step [510/3750],\n",
      "train loss: 0.5215, train acc: 93.7500                             \n",
      "validation loss: 0.4371, validation acc: 91.2800\n",
      "\n",
      "Epoch [1/3], Step [520/3750],\n",
      "train loss: 0.3891, train acc: 93.7500                             \n",
      "validation loss: 0.4369, validation acc: 91.1800\n",
      "\n",
      "Epoch [1/3], Step [530/3750],\n",
      "train loss: 0.7862, train acc: 81.2500                             \n",
      "validation loss: 0.4360, validation acc: 92.1400\n",
      "\n",
      "Epoch [1/3], Step [540/3750],\n",
      "train loss: 0.2324, train acc: 87.5000                             \n",
      "validation loss: 0.4350, validation acc: 91.2700\n",
      "\n",
      "Epoch [1/3], Step [550/3750],\n",
      "train loss: 0.0262, train acc: 100.0000                             \n",
      "validation loss: 0.4341, validation acc: 91.7000\n",
      "\n",
      "Epoch [1/3], Step [560/3750],\n",
      "train loss: 2.1977, train acc: 81.2500                             \n",
      "validation loss: 0.4332, validation acc: 92.0700\n",
      "\n",
      "Epoch [1/3], Step [570/3750],\n",
      "train loss: 0.0926, train acc: 93.7500                             \n",
      "validation loss: 0.4321, validation acc: 92.3700\n",
      "\n",
      "Epoch [1/3], Step [580/3750],\n",
      "train loss: 0.1132, train acc: 93.7500                             \n",
      "validation loss: 0.4311, validation acc: 92.1900\n",
      "\n",
      "Epoch [1/3], Step [590/3750],\n",
      "train loss: 0.7351, train acc: 93.7500                             \n",
      "validation loss: 0.4306, validation acc: 91.6900\n",
      "\n",
      "Epoch [1/3], Step [600/3750],\n",
      "train loss: 0.3665, train acc: 87.5000                             \n",
      "validation loss: 0.4299, validation acc: 90.8000\n",
      "\n",
      "Epoch [1/3], Step [610/3750],\n",
      "train loss: 0.1181, train acc: 93.7500                             \n",
      "validation loss: 0.4290, validation acc: 92.2800\n",
      "\n",
      "Epoch [1/3], Step [620/3750],\n",
      "train loss: 0.1894, train acc: 93.7500                             \n",
      "validation loss: 0.4281, validation acc: 92.5700\n",
      "\n",
      "Epoch [1/3], Step [630/3750],\n",
      "train loss: 0.3159, train acc: 87.5000                             \n",
      "validation loss: 0.4273, validation acc: 91.3000\n",
      "\n",
      "Epoch [1/3], Step [640/3750],\n",
      "train loss: 1.1462, train acc: 75.0000                             \n",
      "validation loss: 0.4266, validation acc: 92.7000\n",
      "\n",
      "Epoch [1/3], Step [650/3750],\n",
      "train loss: 0.0325, train acc: 100.0000                             \n",
      "validation loss: 0.4258, validation acc: 91.1400\n",
      "\n",
      "Epoch [1/3], Step [660/3750],\n",
      "train loss: 0.1829, train acc: 93.7500                             \n",
      "validation loss: 0.4249, validation acc: 91.8800\n",
      "\n",
      "Epoch [1/3], Step [670/3750],\n",
      "train loss: 0.0756, train acc: 100.0000                             \n",
      "validation loss: 0.4240, validation acc: 92.4000\n",
      "\n",
      "Epoch [1/3], Step [680/3750],\n",
      "train loss: 0.1423, train acc: 93.7500                             \n",
      "validation loss: 0.4229, validation acc: 92.6200\n",
      "\n",
      "Epoch [1/3], Step [690/3750],\n",
      "train loss: 0.6040, train acc: 93.7500                             \n",
      "validation loss: 0.4220, validation acc: 92.7800\n",
      "\n",
      "Epoch [1/3], Step [700/3750],\n",
      "train loss: 0.2771, train acc: 87.5000                             \n",
      "validation loss: 0.4210, validation acc: 92.5500\n",
      "\n",
      "Epoch [1/3], Step [710/3750],\n",
      "train loss: 0.3381, train acc: 93.7500                             \n",
      "validation loss: 0.4200, validation acc: 92.8500\n",
      "\n",
      "Epoch [1/3], Step [720/3750],\n",
      "train loss: 0.5703, train acc: 93.7500                             \n",
      "validation loss: 0.4197, validation acc: 90.4100\n",
      "\n",
      "Epoch [1/3], Step [730/3750],\n",
      "train loss: 0.2685, train acc: 93.7500                             \n",
      "validation loss: 0.4191, validation acc: 92.1000\n",
      "\n",
      "Epoch [1/3], Step [740/3750],\n",
      "train loss: 0.3075, train acc: 93.7500                             \n",
      "validation loss: 0.4186, validation acc: 89.2500\n",
      "\n",
      "Epoch [1/3], Step [750/3750],\n",
      "train loss: 0.0792, train acc: 93.7500                             \n",
      "validation loss: 0.4182, validation acc: 91.8600\n",
      "\n",
      "Epoch [1/3], Step [760/3750],\n",
      "train loss: 0.1554, train acc: 93.7500                             \n",
      "validation loss: 0.4174, validation acc: 92.2300\n",
      "\n",
      "Epoch [1/3], Step [770/3750],\n",
      "train loss: 0.4116, train acc: 87.5000                             \n",
      "validation loss: 0.4169, validation acc: 90.0300\n",
      "\n",
      "Epoch [1/3], Step [780/3750],\n",
      "train loss: 0.2984, train acc: 93.7500                             \n",
      "validation loss: 0.4162, validation acc: 92.8200\n",
      "\n",
      "Epoch [1/3], Step [790/3750],\n",
      "train loss: 0.0657, train acc: 100.0000                             \n",
      "validation loss: 0.4154, validation acc: 92.4600\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "starting epoch\n",
      "Epoch [2/3], Step [0/3750],\n",
      "train loss: 0.0956, train acc: 100.0000                             \n",
      "validation loss: 0.4146, validation acc: 92.5200\n",
      "\n",
      "Epoch [2/3], Step [10/3750],\n",
      "train loss: 0.0442, train acc: 100.0000                             \n",
      "validation loss: 0.4138, validation acc: 92.6700\n",
      "\n",
      "Epoch [2/3], Step [20/3750],\n",
      "train loss: 0.0638, train acc: 100.0000                             \n",
      "validation loss: 0.4128, validation acc: 93.2900\n",
      "\n",
      "Epoch [2/3], Step [30/3750],\n",
      "train loss: 0.0416, train acc: 100.0000                             \n",
      "validation loss: 0.4119, validation acc: 91.8500\n",
      "\n",
      "Epoch [2/3], Step [40/3750],\n",
      "train loss: 0.3911, train acc: 81.2500                             \n",
      "validation loss: 0.4112, validation acc: 92.2900\n",
      "\n",
      "Epoch [2/3], Step [50/3750],\n",
      "train loss: 0.3268, train acc: 93.7500                             \n",
      "validation loss: 0.4104, validation acc: 91.9400\n",
      "\n",
      "Epoch [2/3], Step [60/3750],\n",
      "train loss: 0.6482, train acc: 81.2500                             \n",
      "validation loss: 0.4097, validation acc: 92.4500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [70/3750],\n",
      "train loss: 0.2315, train acc: 93.7500                             \n",
      "validation loss: 0.4089, validation acc: 92.3700\n",
      "\n",
      "Epoch [2/3], Step [80/3750],\n",
      "train loss: 0.4180, train acc: 81.2500                             \n",
      "validation loss: 0.4081, validation acc: 92.1200\n",
      "\n",
      "Epoch [2/3], Step [90/3750],\n",
      "train loss: 0.1139, train acc: 100.0000                             \n",
      "validation loss: 0.4073, validation acc: 92.8900\n",
      "\n",
      "Epoch [2/3], Step [100/3750],\n",
      "train loss: 0.1335, train acc: 93.7500                             \n",
      "validation loss: 0.4065, validation acc: 92.3300\n",
      "\n",
      "Epoch [2/3], Step [110/3750],\n",
      "train loss: 0.0990, train acc: 93.7500                             \n",
      "validation loss: 0.4058, validation acc: 92.0500\n",
      "\n",
      "Epoch [2/3], Step [120/3750],\n",
      "train loss: 0.3553, train acc: 93.7500                             \n",
      "validation loss: 0.4053, validation acc: 92.3100\n",
      "\n",
      "Epoch [2/3], Step [130/3750],\n",
      "train loss: 0.3814, train acc: 81.2500                             \n",
      "validation loss: 0.4048, validation acc: 90.8400\n",
      "\n",
      "Epoch [2/3], Step [140/3750],\n",
      "train loss: 0.2844, train acc: 81.2500                             \n",
      "validation loss: 0.4042, validation acc: 91.8500\n",
      "\n",
      "Epoch [2/3], Step [150/3750],\n",
      "train loss: 0.1281, train acc: 93.7500                             \n",
      "validation loss: 0.4038, validation acc: 91.8100\n",
      "\n",
      "Epoch [2/3], Step [160/3750],\n",
      "train loss: 0.1159, train acc: 93.7500                             \n",
      "validation loss: 0.4034, validation acc: 91.8000\n",
      "\n",
      "Epoch [2/3], Step [170/3750],\n",
      "train loss: 0.0704, train acc: 93.7500                             \n",
      "validation loss: 0.4033, validation acc: 90.1600\n",
      "\n",
      "Epoch [2/3], Step [180/3750],\n",
      "train loss: 0.3540, train acc: 81.2500                             \n",
      "validation loss: 0.4031, validation acc: 91.7500\n",
      "\n",
      "Epoch [2/3], Step [190/3750],\n",
      "train loss: 0.0536, train acc: 100.0000                             \n",
      "validation loss: 0.4028, validation acc: 91.4600\n",
      "\n",
      "Epoch [2/3], Step [200/3750],\n",
      "train loss: 0.0461, train acc: 100.0000                             \n",
      "validation loss: 0.4024, validation acc: 92.1600\n",
      "\n",
      "Epoch [2/3], Step [210/3750],\n",
      "train loss: 0.1863, train acc: 93.7500                             \n",
      "validation loss: 0.4022, validation acc: 90.7800\n",
      "\n",
      "Epoch [2/3], Step [220/3750],\n",
      "train loss: 0.2934, train acc: 93.7500                             \n",
      "validation loss: 0.4017, validation acc: 93.1500\n",
      "\n",
      "Epoch [2/3], Step [230/3750],\n",
      "train loss: 0.4010, train acc: 87.5000                             \n",
      "validation loss: 0.4012, validation acc: 91.7900\n",
      "\n",
      "Epoch [2/3], Step [240/3750],\n",
      "train loss: 0.0066, train acc: 100.0000                             \n",
      "validation loss: 0.4008, validation acc: 92.9300\n",
      "\n",
      "Epoch [2/3], Step [250/3750],\n",
      "train loss: 0.2304, train acc: 81.2500                             \n",
      "validation loss: 0.4003, validation acc: 92.0900\n",
      "\n",
      "Epoch [2/3], Step [260/3750],\n",
      "train loss: 0.6515, train acc: 75.0000                             \n",
      "validation loss: 0.4002, validation acc: 90.1600\n",
      "\n",
      "Epoch [2/3], Step [270/3750],\n",
      "train loss: 0.6982, train acc: 87.5000                             \n",
      "validation loss: 0.4000, validation acc: 91.3200\n",
      "\n",
      "Epoch [2/3], Step [280/3750],\n",
      "train loss: 0.1538, train acc: 93.7500                             \n",
      "validation loss: 0.3996, validation acc: 91.5700\n",
      "\n",
      "Epoch [2/3], Step [290/3750],\n",
      "train loss: 0.2088, train acc: 93.7500                             \n",
      "validation loss: 0.3993, validation acc: 89.9600\n",
      "\n",
      "Epoch [2/3], Step [300/3750],\n",
      "train loss: 0.1443, train acc: 93.7500                             \n",
      "validation loss: 0.3992, validation acc: 90.7800\n",
      "\n",
      "Epoch [2/3], Step [310/3750],\n",
      "train loss: 0.3182, train acc: 87.5000                             \n",
      "validation loss: 0.3991, validation acc: 89.9200\n",
      "\n",
      "Epoch [2/3], Step [320/3750],\n",
      "train loss: 0.8497, train acc: 81.2500                             \n",
      "validation loss: 0.3990, validation acc: 91.5800\n",
      "\n",
      "Epoch [2/3], Step [330/3750],\n",
      "train loss: 0.3011, train acc: 93.7500                             \n",
      "validation loss: 0.3990, validation acc: 89.8300\n",
      "\n",
      "Epoch [2/3], Step [340/3750],\n",
      "train loss: 0.0550, train acc: 100.0000                             \n",
      "validation loss: 0.3986, validation acc: 91.6200\n",
      "\n",
      "Epoch [2/3], Step [350/3750],\n",
      "train loss: 0.3535, train acc: 93.7500                             \n",
      "validation loss: 0.3983, validation acc: 91.2900\n",
      "\n",
      "Epoch [2/3], Step [360/3750],\n",
      "train loss: 0.0916, train acc: 100.0000                             \n",
      "validation loss: 0.3978, validation acc: 91.9100\n",
      "\n",
      "Epoch [2/3], Step [370/3750],\n",
      "train loss: 0.3410, train acc: 93.7500                             \n",
      "validation loss: 0.3976, validation acc: 91.0100\n",
      "\n",
      "Epoch [2/3], Step [380/3750],\n",
      "train loss: 0.0887, train acc: 93.7500                             \n",
      "validation loss: 0.3970, validation acc: 92.6000\n",
      "\n",
      "Epoch [2/3], Step [390/3750],\n",
      "train loss: 0.0841, train acc: 100.0000                             \n",
      "validation loss: 0.3964, validation acc: 91.5400\n",
      "\n",
      "Epoch [2/3], Step [400/3750],\n",
      "train loss: 0.5788, train acc: 87.5000                             \n",
      "validation loss: 0.3960, validation acc: 92.6800\n",
      "\n",
      "Epoch [2/3], Step [410/3750],\n",
      "train loss: 0.1461, train acc: 93.7500                             \n",
      "validation loss: 0.3954, validation acc: 93.0500\n",
      "\n",
      "Epoch [2/3], Step [420/3750],\n",
      "train loss: 0.3804, train acc: 87.5000                             \n",
      "validation loss: 0.3948, validation acc: 92.1600\n",
      "\n",
      "Epoch [2/3], Step [430/3750],\n",
      "train loss: 0.1746, train acc: 93.7500                             \n",
      "validation loss: 0.3944, validation acc: 92.1300\n",
      "\n",
      "Epoch [2/3], Step [440/3750],\n",
      "train loss: 0.4130, train acc: 93.7500                             \n",
      "validation loss: 0.3937, validation acc: 93.3500\n",
      "\n",
      "Epoch [2/3], Step [450/3750],\n",
      "train loss: 0.1007, train acc: 93.7500                             \n",
      "validation loss: 0.3931, validation acc: 92.1000\n",
      "\n",
      "Epoch [2/3], Step [460/3750],\n",
      "train loss: 0.0944, train acc: 93.7500                             \n",
      "validation loss: 0.3926, validation acc: 91.9900\n",
      "\n",
      "Epoch [2/3], Step [470/3750],\n",
      "train loss: 0.2184, train acc: 93.7500                             \n",
      "validation loss: 0.3919, validation acc: 93.4400\n",
      "\n",
      "Epoch [2/3], Step [480/3750],\n",
      "train loss: 0.3709, train acc: 87.5000                             \n",
      "validation loss: 0.3913, validation acc: 93.3400\n",
      "\n",
      "Epoch [2/3], Step [490/3750],\n",
      "train loss: 0.1970, train acc: 87.5000                             \n",
      "validation loss: 0.3906, validation acc: 93.6700\n",
      "\n",
      "Epoch [2/3], Step [500/3750],\n",
      "train loss: 0.3183, train acc: 87.5000                             \n",
      "validation loss: 0.3899, validation acc: 93.0700\n",
      "\n",
      "Epoch [2/3], Step [510/3750],\n",
      "train loss: 0.2197, train acc: 93.7500                             \n",
      "validation loss: 0.3893, validation acc: 93.0800\n",
      "\n",
      "Epoch [2/3], Step [520/3750],\n",
      "train loss: 0.0173, train acc: 100.0000                             \n",
      "validation loss: 0.3888, validation acc: 92.4700\n",
      "\n",
      "Epoch [2/3], Step [530/3750],\n",
      "train loss: 0.0113, train acc: 100.0000                             \n",
      "validation loss: 0.3884, validation acc: 92.3700\n",
      "\n",
      "Epoch [2/3], Step [540/3750],\n",
      "train loss: 0.4673, train acc: 93.7500                             \n",
      "validation loss: 0.3878, validation acc: 92.9000\n",
      "\n",
      "Epoch [2/3], Step [550/3750],\n",
      "train loss: 0.6721, train acc: 87.5000                             \n",
      "validation loss: 0.3875, validation acc: 91.6800\n",
      "\n",
      "Epoch [2/3], Step [560/3750],\n",
      "train loss: 0.4168, train acc: 87.5000                             \n",
      "validation loss: 0.3871, validation acc: 92.3800\n",
      "\n",
      "Epoch [2/3], Step [570/3750],\n",
      "train loss: 0.1156, train acc: 100.0000                             \n",
      "validation loss: 0.3867, validation acc: 91.5900\n",
      "\n",
      "Epoch [2/3], Step [580/3750],\n",
      "train loss: 0.3097, train acc: 87.5000                             \n",
      "validation loss: 0.3862, validation acc: 92.5500\n",
      "\n",
      "Epoch [2/3], Step [590/3750],\n",
      "train loss: 0.0690, train acc: 93.7500                             \n",
      "validation loss: 0.3858, validation acc: 92.4900\n",
      "\n",
      "Epoch [2/3], Step [600/3750],\n",
      "train loss: 0.2386, train acc: 87.5000                             \n",
      "validation loss: 0.3854, validation acc: 92.4600\n",
      "\n",
      "Epoch [2/3], Step [610/3750],\n",
      "train loss: 0.0041, train acc: 100.0000                             \n",
      "validation loss: 0.3849, validation acc: 92.9000\n",
      "\n",
      "Epoch [2/3], Step [620/3750],\n",
      "train loss: 0.0216, train acc: 100.0000                             \n",
      "validation loss: 0.3847, validation acc: 91.1900\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [630/3750],\n",
      "train loss: 0.0888, train acc: 100.0000                             \n",
      "validation loss: 0.3843, validation acc: 93.0200\n",
      "\n",
      "Epoch [2/3], Step [640/3750],\n",
      "train loss: 0.0097, train acc: 100.0000                             \n",
      "validation loss: 0.3838, validation acc: 92.9100\n",
      "\n",
      "Epoch [2/3], Step [650/3750],\n",
      "train loss: 0.9366, train acc: 87.5000                             \n",
      "validation loss: 0.3833, validation acc: 93.1100\n",
      "\n",
      "Epoch [2/3], Step [660/3750],\n",
      "train loss: 0.0593, train acc: 100.0000                             \n",
      "validation loss: 0.3827, validation acc: 93.1300\n",
      "\n",
      "Epoch [2/3], Step [670/3750],\n",
      "train loss: 0.0726, train acc: 100.0000                             \n",
      "validation loss: 0.3822, validation acc: 93.4100\n",
      "\n",
      "Epoch [2/3], Step [680/3750],\n",
      "train loss: 0.0534, train acc: 100.0000                             \n",
      "validation loss: 0.3816, validation acc: 94.0000\n",
      "\n",
      "Epoch [2/3], Step [690/3750],\n",
      "train loss: 0.5754, train acc: 81.2500                             \n",
      "validation loss: 0.3809, validation acc: 94.0300\n",
      "\n",
      "Epoch [2/3], Step [700/3750],\n",
      "train loss: 0.2044, train acc: 100.0000                             \n",
      "validation loss: 0.3803, validation acc: 93.9200\n",
      "\n",
      "Epoch [2/3], Step [710/3750],\n",
      "train loss: 0.5210, train acc: 93.7500                             \n",
      "validation loss: 0.3798, validation acc: 92.8000\n",
      "\n",
      "Epoch [2/3], Step [720/3750],\n",
      "train loss: 0.0426, train acc: 100.0000                             \n",
      "validation loss: 0.3795, validation acc: 91.7100\n",
      "\n",
      "Epoch [2/3], Step [730/3750],\n",
      "train loss: 0.7016, train acc: 87.5000                             \n",
      "validation loss: 0.3791, validation acc: 92.8000\n",
      "\n",
      "Epoch [2/3], Step [740/3750],\n",
      "train loss: 0.1669, train acc: 93.7500                             \n",
      "validation loss: 0.3786, validation acc: 93.3900\n",
      "\n",
      "Epoch [2/3], Step [750/3750],\n",
      "train loss: 0.1116, train acc: 93.7500                             \n",
      "validation loss: 0.3783, validation acc: 92.0900\n",
      "\n",
      "Epoch [2/3], Step [760/3750],\n",
      "train loss: 0.2026, train acc: 93.7500                             \n",
      "validation loss: 0.3782, validation acc: 91.1100\n",
      "\n",
      "Epoch [2/3], Step [770/3750],\n",
      "train loss: 0.0792, train acc: 100.0000                             \n",
      "validation loss: 0.3780, validation acc: 92.1400\n",
      "\n",
      "Epoch [2/3], Step [780/3750],\n",
      "train loss: 0.1491, train acc: 93.7500                             \n",
      "validation loss: 0.3779, validation acc: 91.8100\n",
      "\n",
      "Epoch [2/3], Step [790/3750],\n",
      "train loss: 0.0408, train acc: 100.0000                             \n",
      "validation loss: 0.3778, validation acc: 93.4100\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/mnist_centralized_loss.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23576/219431150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23576/219431150.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss for Centralized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# plt.ylim((0, 100))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/mnist_centralized_loss.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3013\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3015\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3017\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2253\u001b[0m                 \u001b[1;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2255\u001b[1;33m                     result = print_method(\n\u001b[0m\u001b[0;32m   2256\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2257\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \"\"\"\n\u001b[0;32m    508\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         mpl.image.imsave(\n\u001b[0m\u001b[0;32m    510\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"upper\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dpi\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1616\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2235\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2236\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2237\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/mnist_centralized_loss.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "\n",
    "class CustomMnistModel(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, num_classes) -> None:\n",
    "\n",
    "        super(CustomMnistModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "#from custom_dataset import CustomRNADatasetCentral\n",
    "#from metrics import accuracy\n",
    "#from fed_utils import return_scaled_weights, sum_scaled_weights, set_layer_weights\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "NUM_EPOCHS = 3\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SHUFFLE_DATASET = True\n",
    "RANDOM_SEED = 42\n",
    "MPS_USED = False\n",
    "\n",
    "##### Centralized Training Procedure #####\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Setting the device to Apple Silicon\n",
    "    if MPS_USED:\n",
    "        device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "    # Create dataset\n",
    "    train_set = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    valid_set = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(\"Dataloaders created\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CustomMnistModel(NUM_FEATURES, NUM_CLASSES)\n",
    "    if MPS_USED:\n",
    "        model.to(device)\n",
    "    print(\"Model created\")\n",
    "    # model.load_state_dict(torch.load('mnist_initial_model.pt'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_steps = len(train_loader)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    valid_loss_min = np.Inf\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    total_steps_taken = 0\n",
    "\n",
    "    print(\"Train loop variables created\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_steps_taken = 1\n",
    "        # correct = 0\n",
    "        # total = 0\n",
    "\n",
    "        print(f'Epoch {epoch}\\n')\n",
    "        print(\"starting epoch\")\n",
    "        for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            if batch_idx < 500:\n",
    "                if MPS_USED:\n",
    "                    data_ = data_.to(device)\n",
    "                    target_ = target_.to(device)\n",
    "                \n",
    "                # print(data_.device)\n",
    "\n",
    "                ### Fwd pass\n",
    "                outputs = model(data_.view(-1, 28*28))\n",
    "\n",
    "                ### Gradient calc\n",
    "                loss = criterion(outputs, target_)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                ### Print stats\n",
    "                running_loss += loss.item()\n",
    "                _, pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==target_).item()\n",
    "                total += target_.size(0)\n",
    "                \n",
    "                \n",
    "                # if total_steps_taken%5==0:\n",
    "                #     scheduler.step()\n",
    "                        \n",
    "                train_acc.append(100*correct/total)\n",
    "                train_loss.append(loss.item())\n",
    "                # print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "\n",
    "                ## Validate Model Accuracy\n",
    "\n",
    "                batch_loss = 0\n",
    "                total_t = 0\n",
    "                correct_t = 0\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for data_t, target_t in (valid_loader):\n",
    "\n",
    "                        ### To device\n",
    "                        if MPS_USED:\n",
    "                            data_t = data_t.to(device)\n",
    "                            target_t = target_t.to(device)\n",
    "\n",
    "                        ### Fwd pass\n",
    "                        outputs_t = model(data_t.view(-1, 28*28))\n",
    "\n",
    "                        ### Print Stats\n",
    "                        loss_t = criterion(outputs_t, target_t)\n",
    "                        batch_loss += loss_t.item()\n",
    "                        _, pred_t = torch.max(outputs_t, dim=1)\n",
    "                        correct_t += torch.sum(pred_t==target_t).item()\n",
    "                        total_t += target_t.size(0)\n",
    "                    \n",
    "                    val_acc.append(100*correct_t/total_t)\n",
    "                    val_loss.append(batch_loss/len(valid_loader))\n",
    "                    network_learned = batch_loss < valid_loss_min\n",
    "                    # print(f\"validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n\")\n",
    "\n",
    "                    if network_learned:\n",
    "                        valid_loss_min = batch_loss\n",
    "                        torch.save(model.state_dict(), 'classification_model_mnist.pt')\n",
    "                        # print('Saving current model due to improvement')\n",
    "\n",
    "                    if (batch_idx % 10 == 0):\n",
    "                        print(f'Epoch [{epoch}/{NUM_EPOCHS}], Step [{batch_idx}/{total_steps}],\\ntrain loss: {train_loss[-1]:.4f}, train acc: {(100 * correct / total):.4f} \\\n",
    "                            \\nvalidation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                ### Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                epoch_steps_taken += 1\n",
    "                total_steps_taken += 1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.array([x for x in range(len(train_loss))]), train_loss, color='r')\n",
    "    plt.plot(np.array([x for x in range(len(val_loss))]), val_loss, color='g')\n",
    "    plt.title(\"Loss for Centralized\")\n",
    "    # plt.ylim((0, 100))\n",
    "    plt.savefig(\"mnist_centralized_loss.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.array([x for x in range(len(train_acc))]), train_acc, color='r')\n",
    "    plt.plot(np.array([x for x in range(len(val_acc))]), val_acc, color='g')\n",
    "    plt.title(\"Accuracy for Centralized\")\n",
    "    # plt.ylim((0, 100))\n",
    "    plt.savefig(\"results/mnist_centralized_acc.png\")\n",
    "\n",
    "    train_loss_dict = pd.DataFrame({\"loss\":train_loss})\n",
    "    train_acc_dict = pd.DataFrame({\"loss\":train_acc})\n",
    "    val_loss_dict = pd.DataFrame({\"loss\":val_loss})\n",
    "    val_acc_dict = pd.DataFrame({\"loss\":val_acc})\n",
    "\n",
    "    train_acc_dict.to_csv(\"mnist_train_acc.csv\")\n",
    "    train_loss_dict.to_csv(\"mnist_train_loss.csv\")\n",
    "    val_acc_dict.to_csv(\"mnist_val_acc.csv\")\n",
    "    val_loss_dict.to_csv(\"mnist_val_loss.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "main()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b6fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
